{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchviz torch pyarrow==11.0.0 category_encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOcRofHstIOY",
        "outputId": "3a8223e9-3bc1-4fdf-8b00-f068bff7e5c7"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model, metrics, model_selection, svm, neighbors, preprocessing, datasets, pipeline, ensemble\n",
        "from sklearn.feature_selection import SelectKBest, SelectFromModel, GenericUnivariateSelect\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.feature_selection import chi2, mutual_info_classif\n",
        "from sklearn.datasets import load_diabetes, make_classification\n",
        "from sklearn.datasets import load_breast_cancer, fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.covariance import empirical_covariance\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn.feature_selection as fs\n",
        "import sklearn.preprocessing as skpr\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import category_encoders as ce\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import compose\n",
        "from sklearn.svm import SVC\n",
        "import lightgbm as lgb\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import torch\n",
        "import copy\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCSZsPuwLKyE"
      },
      "source": [
        "#### Кодирование признаков (author: github.com/dmforit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsBmAT8uLJLK"
      },
      "outputs": [],
      "source": [
        "class CircularEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, limits=None, fit_replace=True, tol=1e-8):\n",
        "        self.limits = limits\n",
        "        self.fit_replace = fit_replace\n",
        "        self.tol = tol\n",
        "        self._shape = (0, 0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Circular Encoder\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Circular Encoder\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        # Shape setting\n",
        "        self._shape = self.__set_shape(X)\n",
        "\n",
        "        # Defining limit\n",
        "        if self.fit_replace:\n",
        "            self.limits = self.__set_limits(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        if self.limits is None:\n",
        "            return copy.deepcopy(X)\n",
        "\n",
        "        # column_names only for DataFrame\n",
        "        column_names = None\n",
        "\n",
        "        # cast to numpy array\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_ndarray = X.to_numpy().reshape(self._shape)\n",
        "            column_names = np.zeros(2 * X.columns.shape[0], dtype=object)\n",
        "        else:\n",
        "            X_ndarray = X.reshape(self._shape)\n",
        "\n",
        "        # main encoding\n",
        "        result_sin = np.sin((2 * np.pi * X_ndarray) / self.limits)\n",
        "        result_cos = np.cos((2 * np.pi * X_ndarray) / self.limits)\n",
        "\n",
        "        result_sin[np.abs(result_sin) < self.tol] = 0.0\n",
        "        result_cos[np.abs(result_cos) < self.tol] = 0.0\n",
        "\n",
        "        # combine encoded arrays\n",
        "        result = np.zeros((self._shape[0], self._shape[1] * 2))\n",
        "        result[:, np.arange(0, result.shape[1], 2)] = result_sin\n",
        "        result[:, np.arange(1, result.shape[1], 2)] = result_cos\n",
        "\n",
        "        # set column_names names and return result\n",
        "        if column_names is not None:\n",
        "            column_names[np.arange(0, result.shape[1], 2)] = np.array([f'sin_{col}' for col in X.columns])\n",
        "            column_names[np.arange(1, result.shape[1], 2)] = np.array([f'cos_{col}' for col in X.columns])\n",
        "            return pd.DataFrame(result, columns=column_names).infer_objects()\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "    @staticmethod\n",
        "    def __set_shape(X):\n",
        "        if len(X.shape) == 1:\n",
        "            return (X.shape[0], 1)\n",
        "        elif len(X.shape) > 2:\n",
        "            raise ValueError(f\"You need 2 dimensions instead of {len(X.shape)}\")\n",
        "        else:\n",
        "            return X.shape\n",
        "\n",
        "    @staticmethod\n",
        "    def __set_limits(X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return np.max(np.abs(X.to_numpy()), axis=0) + 1\n",
        "        else:\n",
        "            return np.max(np.abs(X), axis=0) + 1\n",
        "\n",
        "\n",
        "class DateTimeEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols=None, fit_replace=False,\n",
        "                 drop=True, min_rescale=False, fast_mode=True,\n",
        "                 confidence_level=0.99, worst_proportion=0.01):\n",
        "        self.cols = np.array(cols) if cols is not None else None\n",
        "        self.fit_replace = fit_replace\n",
        "        self.drop = drop\n",
        "        self.min_rescale = min_rescale\n",
        "\n",
        "        # stochastic approach\n",
        "        eps = 1e-6\n",
        "        confidence_level = max(eps, min(confidence_level, 1 - eps))\n",
        "        worst_proportion = max(eps, min(worst_proportion, 1 - eps))\n",
        "        self.fast_random_size = math.ceil(math.log(1 - confidence_level) / math.log(1 - worst_proportion))\n",
        "        self.fast_mode = fast_mode\n",
        "\n",
        "        # encoders for months, days and hours\n",
        "        self.encoders = {'months': CircularEncoder(limits=[12], fit_replace=False),\n",
        "                         'days': CircularEncoder(limits=[30], fit_replace=False),\n",
        "                         'hours': CircularEncoder(limits=[24], fit_replace=False)}\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"DateTimeEncoder\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"DateTimeEncoder\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.cols is None or self.fit_replace:\n",
        "            if self.fast_mode and self.fast_random_size < X.shape[0]:\n",
        "                self.cols = self.define_cols_fast(X)\n",
        "            else:\n",
        "                self.cols = self.define_cols(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        try:\n",
        "            if self.cols is None or not self.cols.shape[0]:\n",
        "                return copy.deepcopy(X)\n",
        "        except IndexError:\n",
        "            raise IndexError(\"cols should be a numpy array\")\n",
        "\n",
        "        # save self.cols and get indices for X\n",
        "        saved_cols = self.cols.copy()\n",
        "        self.cols = self.cols_to_numeric(X)\n",
        "\n",
        "        if not self.cols.shape[0]:\n",
        "            self.cols = saved_cols\n",
        "            return copy.deepcopy(X)\n",
        "\n",
        "        column_names = None\n",
        "\n",
        "        # cast to pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            column_names = X.columns.to_numpy(dtype=object).copy()\n",
        "            X_df = X\n",
        "        else:\n",
        "            X_df = pd.DataFrame(X)\n",
        "\n",
        "        # shift for dropping columns\n",
        "        shift = 0 if self.drop else 1\n",
        "\n",
        "        # creating an empty result matrix\n",
        "        result = np.zeros((X_df.shape[0], X_df.shape[1] + (6 + shift) * len(self.cols)), dtype=object)\n",
        "        column_names_zeros = np.zeros(X_df.shape[1] + (6 + shift) * len(self.cols), dtype=object)\n",
        "\n",
        "        # creating indices for fast numpy operation\n",
        "        modified_inds = np.array([self.cols[i] + i * (6 + shift) for i in range(len(self.cols))])\n",
        "        not_cols = list(set(np.arange(X.shape[1])) - set(self.cols))\n",
        "        unmodified_inds = np.array([i + (7 + shift) * (not_cols[i] - i) for i in range(len(not_cols))])\n",
        "\n",
        "        # setting not date columns\n",
        "        if unmodified_inds.shape[0] > 0:\n",
        "            result[:, unmodified_inds] = X_df.to_numpy()[:, not_cols]\n",
        "            if not self.drop:\n",
        "                result[:, modified_inds] = X_df.to_numpy()[:, self.cols]\n",
        "            if column_names is not None:\n",
        "                column_names_zeros[unmodified_inds] = column_names[not_cols]\n",
        "                if not self.drop:\n",
        "                    column_names_zeros[modified_inds] = column_names[self.cols]\n",
        "\n",
        "        # for fast mode\n",
        "        preventive_delete = []\n",
        "\n",
        "        # tranform each datetime column\n",
        "        for num, col in enumerate(self.cols):\n",
        "            transformed_column = self.column_transform(X_df.iloc[:, col])\n",
        "            # for fast mode\n",
        "            if transformed_column is None:\n",
        "                result[:, modified_inds[num] + 6 + shift] = X_df.iloc[:, col]\n",
        "                column_names_zeros[modified_inds[num] + 6 + shift] = X_df.iloc[:, col].name\n",
        "                preventive_delete += list(np.arange(modified_inds[num] + shift, modified_inds[num] + 6 + shift))\n",
        "                continue\n",
        "            result[:, np.arange(modified_inds[num] + shift, modified_inds[num] + 7 + shift)] = transformed_column.to_numpy()\n",
        "            column_names_zeros[np.arange(modified_inds[num] + shift, modified_inds[num] + 7 + shift)] = transformed_column.columns.to_numpy(dtype=object).copy()\n",
        "\n",
        "        # delete unsuccessful transformations\n",
        "        if len(preventive_delete) > 0:\n",
        "            result = np.delete(result, preventive_delete, 1)\n",
        "            column_names_zeros = np.delete(column_names_zeros, preventive_delete)\n",
        "\n",
        "        # self.cols backup\n",
        "        self.cols = saved_cols\n",
        "\n",
        "        if column_names is not None:\n",
        "            # saving dtypes\n",
        "            saved_dtypes = X_df.dtypes.to_dict()\n",
        "            unmodified_dtypes = dict([(X.columns[col], saved_dtypes[X.columns[col]]) for col in not_cols])\n",
        "\n",
        "            return pd.DataFrame(result, columns=column_names_zeros).infer_objects().astype(dtype=unmodified_dtypes)\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "    def column_transform(self, column):\n",
        "        column_name = None\n",
        "\n",
        "        # cast to pandas Series\n",
        "        if not isinstance(column, pd.Series):\n",
        "            pd_column = copy.deepcopy(pd.Series(column))\n",
        "        else:\n",
        "            pd_column = copy.deepcopy(column)\n",
        "            column_name = column.name\n",
        "\n",
        "        try:\n",
        "            pd_column = pd.to_datetime(pd_column.astype(str))\n",
        "        except (ValueError, TypeError):\n",
        "            return None\n",
        "\n",
        "        # rescaling by the start time\n",
        "        if self.min_rescale:\n",
        "            pd_column = pd_column.astype(int)\n",
        "            min_seconds = pd_column.min()\n",
        "            pd_column = pd_column.apply(lambda val: val - min_seconds)\n",
        "            pd_column = pd.to_datetime(pd_column)\n",
        "\n",
        "        # defining main columns\n",
        "        years = pd_column.dt.year.to_numpy()\n",
        "        months = self.encoders['months'].fit_transform(pd_column.dt.month.to_numpy())\n",
        "        days = self.encoders['days'].fit_transform(pd_column.dt.day.to_numpy())\n",
        "        hours = self.encoders['hours'].fit_transform(pd_column.dt.hour.to_numpy())\n",
        "        result = np.array([years, months.T[0], months.T[1], days.T[0],\n",
        "                           days.T[1], hours.T[0], hours.T[1]]).T\n",
        "\n",
        "        # cast to the original type\n",
        "        if column_name is not None:\n",
        "            column_names = np.array([f'{column_name}_year', f'{column_name}_month_sin',\n",
        "                                     f'{column_name}_month_cos', f'{column_name}_day_sin',\n",
        "                                     f'{column_name}_day_cos', f'{column_name}_hour_sin',\n",
        "                                     f'{column_name}_hour_cos'])\n",
        "            result = pd.DataFrame(result, columns=column_names).infer_objects()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def cols_to_numeric(self, X):\n",
        "        if self.cols is None or not self.cols.shape[0]:\n",
        "            return np.array([])\n",
        "\n",
        "        inds = self.cols.copy()\n",
        "\n",
        "        try:\n",
        "            inds = inds.astype(int)\n",
        "            return inds\n",
        "        except (ValueError, TypeError):\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                return np.array([])\n",
        "            else:\n",
        "                columns_array = X.columns.to_numpy(dtype=object).copy()\n",
        "                names_dict = dict((columns_array[i], i) for i in range(columns_array.shape[0]))\n",
        "                inds = np.array([names_dict[name] for name in inds if name in names_dict])\n",
        "                return inds\n",
        "\n",
        "    def define_cols(self, X):\n",
        "        # cast to pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_df = X\n",
        "        else:\n",
        "            X_df = pd.DataFrame(X)\n",
        "\n",
        "        # check all the columns\n",
        "        datetime_cols = []\n",
        "        for num, col in enumerate(X_df):\n",
        "            if not pd.api.types.is_categorical_dtype(X_df[col]) and not pd.api.types.is_numeric_dtype(X_df[col]):\n",
        "                if self.is_datetime(X_df[col].iloc[np.random.randint(0, X_df.shape[0])]):\n",
        "                    datetime_cols.append(num)\n",
        "        return np.array(datetime_cols)\n",
        "\n",
        "    def define_cols_fast(self, X):\n",
        "        # cast to pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_df = X\n",
        "        else:\n",
        "            X_df = pd.DataFrame(X)\n",
        "\n",
        "        # check all the columns\n",
        "        datetime_cols = []\n",
        "        for num, col in enumerate(X_df):\n",
        "            if not pd.api.types.is_categorical_dtype(X_df[col]) and not pd.api.types.is_numeric_dtype(X_df[col]):\n",
        "                append_need = True\n",
        "                for i in range(self.fast_random_size):\n",
        "                    if not self.is_datetime(X_df[col].iloc[np.random.randint(0, X_df.shape[0])]):\n",
        "                        append_need = False\n",
        "                        break\n",
        "                if append_need:\n",
        "                    datetime_cols.append(num)\n",
        "        return np.array(datetime_cols)\n",
        "\n",
        "    def get_cols(self):\n",
        "        if self.cols is not None:\n",
        "            return self.cols.copy()\n",
        "        else:\n",
        "            return np.array([])\n",
        "\n",
        "    def is_datetime(self, col):\n",
        "        arg = None\n",
        "        if not isinstance(col, pd.Series):\n",
        "            arg = col\n",
        "        else:\n",
        "            arg = col.astype(str)\n",
        "\n",
        "        try:\n",
        "            pd.to_datetime(arg, errors='raise')\n",
        "            return True\n",
        "        except (ValueError, TypeError):\n",
        "            return False\n",
        "\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols=None, fit_replace=False, encoder='binary',\n",
        "                 category_rate=0.1, rated_search=True, fast_mode=True,\n",
        "                 confidence_level=0.99, worst_proportion=0.01, **encoder_params):\n",
        "\n",
        "        self._hashing_enc_name = 'hashing'\n",
        "        self._encoders_list = {'onehot': ce.OneHotEncoder,\n",
        "                               'target_loo': ce.LeaveOneOutEncoder,\n",
        "                               self._hashing_enc_name: ce.HashingEncoder,\n",
        "                               'binary': ce.BinaryEncoder}\n",
        "        self._target_encoders_list = np.array(['target_loo'])\n",
        "\n",
        "        # stochastic approach\n",
        "        eps = 1e-6\n",
        "        confidence_level = max(eps, min(confidence_level, 1 - eps))\n",
        "        worst_proportion = max(eps, min(worst_proportion, 1 - eps))\n",
        "        self.fast_random_size = math.ceil(math.log(1 - confidence_level) / math.log(1 - worst_proportion))\n",
        "        self.fast_mode = fast_mode\n",
        "\n",
        "        self.cols = np.array(cols) if cols is not None else None\n",
        "        self.fit_replace = fit_replace\n",
        "        self.encoder_name = encoder\n",
        "        self.category_rate = category_rate\n",
        "        self.rated_search = rated_search\n",
        "        self.encoder_params = encoder_params\n",
        "        self._encoder = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"CategoricalEncoder\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"CategoricalEncoder\"\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        # defining categorical columns\n",
        "        if self.cols is None or self.fit_replace:\n",
        "            self.cols = self.define_cols(X)\n",
        "\n",
        "        try:\n",
        "            if not self.cols.shape[0]:\n",
        "                return self\n",
        "        except IndexError:\n",
        "            raise IndexError(\"cols should be a numpy array\")\n",
        "\n",
        "        # save self.cols and get indices for X\n",
        "        saved_cols = self.cols.copy()\n",
        "        self.cols = self.cols_to_numeric(X)\n",
        "\n",
        "        if not self.cols.shape[0]:\n",
        "            self.cols = saved_cols\n",
        "            return self\n",
        "\n",
        "        # defining the encoder\n",
        "        self.encoder_params['cols'] = X.columns[self.cols] if isinstance(X, pd.DataFrame) else self.cols\n",
        "        self._encoder = self._encoders_list[self.encoder_name](**self.encoder_params)\n",
        "\n",
        "        # transforming y if it's not numeric\n",
        "        y_copy = None\n",
        "\n",
        "        if y is not None:\n",
        "            y_copy = copy.deepcopy(y)\n",
        "            if not self.is_y_approved(y_copy):\n",
        "                y_copy = skpr.LabelEncoder().fit_transform(y_copy)\n",
        "\n",
        "        # fitting the encoder\n",
        "        self._encoder.fit(pd.DataFrame(X), y_copy, **fit_params)\n",
        "\n",
        "        # self.cols backup\n",
        "        self.cols = saved_cols\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        try:\n",
        "            if self.cols is None or not self.cols.shape[0] or self._encoder is None:\n",
        "                return copy.deepcopy(X)\n",
        "        except IndexError:\n",
        "            raise IndexError(\"cols should be a numpy array\")\n",
        "\n",
        "        # save self.cols and get indices for X\n",
        "        saved_cols = self.cols.copy()\n",
        "        self.cols = self.cols_to_numeric(X)\n",
        "\n",
        "        if not self.cols.shape[0]:\n",
        "            self.cols = saved_cols\n",
        "            return copy.deepcopy(X)\n",
        "\n",
        "        # transforming y if it's not numeric\n",
        "        y_copy = None\n",
        "\n",
        "        if y is not None:\n",
        "            y_copy = copy.deepcopy(y)\n",
        "            if not self.is_y_approved(y_copy):\n",
        "                y_copy = skpr.LabelEncoder().fit_transform(y_copy)\n",
        "\n",
        "        # checking whether it's a target encoder or not\n",
        "        if self.encoder_name in self._target_encoders_list:\n",
        "            result = self._encoder.transform(pd.DataFrame(X), y_copy, override_return_df=True)\n",
        "        else:\n",
        "            result = self._encoder.transform(pd.DataFrame(X), override_return_df=True)\n",
        "\n",
        "        # adjusting the column names\n",
        "        cols_before = self._encoder.get_feature_names_in()\n",
        "        cols_after = self._encoder.get_feature_names_out()\n",
        "        new_columns = self._rename_transformed_cols(cols_before, cols_after)\n",
        "\n",
        "        result = result.rename(columns={cols_after[i]: new_columns[i] for i in range(cols_after.shape[0])})\n",
        "\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # saving X dtypes\n",
        "            not_cols = list(set(np.arange(X.shape[1])) - set(self.cols))\n",
        "            saved_dtypes = X.dtypes.to_dict()\n",
        "            unmodified_dtypes = dict([(X.columns[col], saved_dtypes[X.columns[col]]) for col in not_cols])\n",
        "\n",
        "            # self.cols backup\n",
        "            self.cols = saved_cols\n",
        "\n",
        "            return result.astype(dtype=unmodified_dtypes)\n",
        "        else:\n",
        "            # self.cols backup\n",
        "            self.cols = saved_cols\n",
        "\n",
        "            return result.to_numpy()\n",
        "\n",
        "    def fit_transform(self, X, y=None, **fit_params):\n",
        "        # transforming y if it's not numeric\n",
        "        y_copy = None\n",
        "\n",
        "        if y is not None:\n",
        "            y_copy = copy.deepcopy(y)\n",
        "            if not self.is_y_approved(y_copy):\n",
        "                y_copy = skpr.LabelEncoder().fit_transform(y_copy)\n",
        "\n",
        "        self.fit(X, y_copy, **fit_params)\n",
        "        return self.transform(X, y_copy)\n",
        "\n",
        "    def _rename_transformed_cols(self, before, after):\n",
        "        if after is None:\n",
        "            return np.array([])\n",
        "        if self._encoder is None:\n",
        "            return copy.deepcopy(after)\n",
        "\n",
        "        result = copy.deepcopy(after)\n",
        "\n",
        "        # hashing encoder unique renaming\n",
        "        if self.encoder_name == self._hashing_enc_name:\n",
        "            for i in range(self._encoder.n_components):\n",
        "                result[i] = f'{self._hashing_enc_name}_{i}'\n",
        "        else:\n",
        "            # getting the columns transformed\n",
        "            set_before = set(before)\n",
        "            set_after = set(after)\n",
        "            sample_names = set_before.intersection(set_after)\n",
        "\n",
        "            # checking whether set is not empty\n",
        "            if bool(sample_names):\n",
        "                # renaming\n",
        "                for num, col in enumerate(result):\n",
        "                    if col not in sample_names:\n",
        "                        result[num] = f'{self.encoder_name}_{result[num]}'\n",
        "\n",
        "        return result\n",
        "\n",
        "    def is_y_approved(self, y):\n",
        "        try:\n",
        "            y.astype(float)\n",
        "            return True\n",
        "        except (ValueError, TypeError):\n",
        "            return False\n",
        "\n",
        "    def cols_to_numeric(self, X):\n",
        "        if self.cols is None or not self.cols.shape[0]:\n",
        "            return np.array([])\n",
        "\n",
        "        inds = self.cols.copy()\n",
        "\n",
        "        try:\n",
        "            inds = inds.astype(int)\n",
        "            return inds\n",
        "        except (ValueError, TypeError):\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                return np.array([])\n",
        "            else:\n",
        "                columns_array = X.columns.to_numpy(dtype=object).copy()\n",
        "                names_dict = dict((columns_array[i], i) for i in range(columns_array.shape[0]))\n",
        "                inds = np.array([names_dict[name] for name in inds if name in names_dict])\n",
        "                return inds\n",
        "\n",
        "    def define_cols(self, X):\n",
        "        # cast to pandas DataFrame with inferring object types\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X_df = pd.DataFrame(X).infer_objects()\n",
        "        else:\n",
        "            X_df = X.infer_objects()\n",
        "\n",
        "        # check all the columns\n",
        "        category_cols = []\n",
        "        for num, col in enumerate(X_df):\n",
        "            # if dtype is 'category'\n",
        "            if pd.api.types.is_categorical_dtype(X_df[col]):\n",
        "                category_cols.append(num)\n",
        "            # checking object type (strings)\n",
        "            elif pd.api.types.is_object_dtype(X_df[col]):\n",
        "                # stochastic approach\n",
        "                if self.fast_mode:\n",
        "                    append_need = True\n",
        "                    for i in range(self.fast_random_size):\n",
        "                        if not self.is_one_word(X_df[col].iloc[np.random.randint(0, X_df.shape[0])]):\n",
        "                            append_need = False\n",
        "                            break\n",
        "                    if append_need:\n",
        "                        category_cols.append(num)\n",
        "                else:\n",
        "                    # basic approach\n",
        "                    if np.all(np.vectorize(self.is_one_word)(X_df[col])):\n",
        "                        category_cols.append(num)\n",
        "            # checking numeric columns\n",
        "            elif self.rated_search and pd.api.types.is_numeric_dtype(X_df[col]) and not pd.api.types.is_float_dtype(X_df[col]):\n",
        "                if X_df[col].nunique() < self.category_rate * X_df.shape[0]:\n",
        "                    category_cols.append(num)\n",
        "        return np.array(category_cols)\n",
        "\n",
        "    def is_one_word(self, s):\n",
        "        if s is None or not isinstance(s, str):\n",
        "            return False\n",
        "\n",
        "        stripped_string = s.strip()\n",
        "        if not stripped_string or ' ' in stripped_string:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def get_cols(self):\n",
        "        if self.cols is not None:\n",
        "            return self.cols.copy()\n",
        "        else:\n",
        "            return np.array([])\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder_name\n",
        "\n",
        "    def get_available_encoders(self):\n",
        "        return np.array(list(self._encoders_list.keys()))\n",
        "\n",
        "\n",
        "class NumericalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols=None, fit_replace=False, encoder='standard', numeric_rate=0.1,\n",
        "                 rated_search=True, only_float=True, **encoder_params):\n",
        "\n",
        "        self._encoders_list = {'standard': skpr.StandardScaler,\n",
        "                          'min_max': skpr.MinMaxScaler,\n",
        "                          'normalizer': skpr.Normalizer,\n",
        "                          'max_abs': skpr.MaxAbsScaler}\n",
        "\n",
        "        self.only_float = only_float\n",
        "        self.cols = np.array(cols) if cols is not None else None\n",
        "        self.fit_replace = fit_replace\n",
        "        self.encoder_name = encoder\n",
        "        self.numeric_rate = numeric_rate\n",
        "        self.rated_search = rated_search\n",
        "        self.encoder_params = encoder_params\n",
        "        self._encoder = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"NumericalEncoder\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"NumericalEncoder\"\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        # defining numerical columns\n",
        "        if self.cols is None or self.fit_replace:\n",
        "            self.cols = self.define_cols(X)\n",
        "\n",
        "        try:\n",
        "            if not self.cols.shape[0]:\n",
        "                return self\n",
        "        except IndexError:\n",
        "            raise IndexError(\"cols should be a numpy array\")\n",
        "\n",
        "        # save self.cols and get indices for X\n",
        "        saved_cols = self.cols.copy()\n",
        "        self.cols = self.cols_to_numeric(X)\n",
        "\n",
        "        if not self.cols.shape[0]:\n",
        "            self.cols = saved_cols\n",
        "            return self\n",
        "\n",
        "        # defining the encoder\n",
        "        self._encoder = self._encoders_list[self.encoder_name](**self.encoder_params)\n",
        "\n",
        "        # fitting the encoder\n",
        "        X_ndarray = X.to_numpy()[:, self.cols] if isinstance(X, pd.DataFrame) else X[:, self.cols]\n",
        "        self._encoder.fit(X_ndarray, y, **fit_params)\n",
        "\n",
        "        # self.cols backup\n",
        "        self.cols = saved_cols\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        try:\n",
        "            if self.cols is None or not self.cols.shape[0] or self._encoder is None:\n",
        "                return copy.deepcopy(X)\n",
        "        except IndexError:\n",
        "            raise IndexError(\"cols should be a numpy array\")\n",
        "\n",
        "        # save self.cols and get indices for X\n",
        "        saved_cols = self.cols.copy()\n",
        "        self.cols = self.cols_to_numeric(X)\n",
        "\n",
        "        if not self.cols.shape[0]:\n",
        "            self.cols = saved_cols\n",
        "            return copy.deepcopy(X)\n",
        "\n",
        "        column_names = None\n",
        "\n",
        "        # cast to numpy array\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_ndarray = X.iloc[:, self.cols].to_numpy().astype(float)\n",
        "            column_names = X.columns.to_numpy(dtype=object).copy()\n",
        "        else:\n",
        "            X_ndarray = X[:, self.cols].astype(float)\n",
        "\n",
        "        # getting the resulting column names\n",
        "        if column_names is not None:\n",
        "            column_names[self.cols] = np.array([f'{self.encoder_name}_{name}' for name in column_names[self.cols]])\n",
        "\n",
        "        # only numerical columns here\n",
        "        result = self._encoder.transform(X_ndarray)\n",
        "\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # saving X dtypes\n",
        "            not_cols = list(set(np.arange(X.shape[1])) - set(self.cols))\n",
        "            saved_dtypes = X.dtypes.to_dict()\n",
        "            unmodified_dtypes = dict([(X.columns[col], saved_dtypes[X.columns[col]]) for col in not_cols])\n",
        "\n",
        "            X_ndarray = copy.deepcopy(X.to_numpy(dtype=object))\n",
        "            X_ndarray[:, self.cols] = result\n",
        "            result = pd.DataFrame(X_ndarray, columns=column_names).astype(dtype=unmodified_dtypes).infer_objects()\n",
        "        else:\n",
        "            X_ndarray = copy.deepcopy(X)\n",
        "            X_ndarray[:, self.cols] = result\n",
        "            result = X_ndarray\n",
        "\n",
        "        # self.cols backup\n",
        "        self.cols = saved_cols\n",
        "\n",
        "        return result\n",
        "\n",
        "    def define_cols(self, X):\n",
        "        # cast to pandas DataFrame with inferring object types\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X_df = pd.DataFrame(X).infer_objects()\n",
        "        else:\n",
        "            X_df = X.infer_objects()\n",
        "\n",
        "        # check all the columns\n",
        "        numeric_cols = []\n",
        "        for num, col in enumerate(X_df):\n",
        "            # if dtype is 'numeric'\n",
        "            if pd.api.types.is_numeric_dtype(X_df[col]):\n",
        "                # appending if float\n",
        "                if pd.api.types.is_float_dtype(X_df[col]):\n",
        "                    numeric_cols.append(num)\n",
        "                elif not self.only_float:\n",
        "                    # checking rated_search\n",
        "                    if self.rated_search:\n",
        "                        if X_df[col].nunique() >= self.numeric_rate * X_df.shape[0]:\n",
        "                            numeric_cols.append(num)\n",
        "                    else:\n",
        "                        numeric_cols.append(num)\n",
        "        return np.array(numeric_cols)\n",
        "\n",
        "    def cols_to_numeric(self, X):\n",
        "        if self.cols is None or not self.cols.shape[0]:\n",
        "            return np.array([])\n",
        "\n",
        "        inds = self.cols.copy()\n",
        "\n",
        "        try:\n",
        "            inds = inds.astype(int)\n",
        "            return inds\n",
        "        except (ValueError, TypeError):\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                return np.array([])\n",
        "            else:\n",
        "                columns_array = X.columns.to_numpy(dtype=object).copy()\n",
        "                names_dict = dict((columns_array[i], i) for i in range(columns_array.shape[0]))\n",
        "                inds = np.array([names_dict[name] for name in inds if name in names_dict])\n",
        "                return inds\n",
        "\n",
        "    def get_cols(self):\n",
        "        if self.cols is not None:\n",
        "            return self.cols.copy()\n",
        "        else:\n",
        "            return np.array([])\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder_name\n",
        "\n",
        "    def get_available_encoders(self):\n",
        "        return np.array(list(self._encoders_list.keys()))\n",
        "\n",
        "\n",
        "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, categorical_args=None,\n",
        "                 numerical_args=None, datetime_args=None, categorical_enabled=True,\n",
        "                 numerical_enabled=True, datetime_enabled=True, target_encoder='label',\n",
        "                 target_encoding=False):\n",
        "        self.target_encoder = target_encoder\n",
        "        self.target_encoding = target_encoding\n",
        "        self._target_encoders_list = {'label': skpr.LabelEncoder}\n",
        "\n",
        "        self.categorical_args = categorical_args if categorical_args is not None else dict()\n",
        "        self.numerical_args = numerical_args if numerical_args is not None else dict()\n",
        "        self.datetime_args = datetime_args if datetime_args is not None else dict()\n",
        "        self.categorical_enc = CategoricalEncoder(**self.categorical_args)\n",
        "        self.numerical_enc = NumericalEncoder(**self.numerical_args)\n",
        "        self.datetime_enc = DateTimeEncoder(**self.datetime_args)\n",
        "\n",
        "        self.categorical_enabled = categorical_enabled\n",
        "        self.numerical_enabled = numerical_enabled\n",
        "        self.datetime_enabled = datetime_enabled\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        if self.categorical_enabled:\n",
        "            self.categorical_enc.fit(X, y, **fit_params)\n",
        "\n",
        "        if self.numerical_enabled:\n",
        "            self.numerical_enc.fit(X, y, **fit_params)\n",
        "\n",
        "        if self.datetime_enabled:\n",
        "            self.datetime_enc.fit(X, y, **fit_params)\n",
        "\n",
        "        cat_cols = self.cols_to_numeric(X, self.categorical_enc.get_cols())\n",
        "        num_cols = self.cols_to_numeric(X, self.numerical_enc.get_cols())\n",
        "        dt_cols = self.cols_to_numeric(X, self.datetime_enc.get_cols())\n",
        "\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_columns = X.columns.to_numpy(dtype=object).copy()\n",
        "            self.categorical_enc.cols = X_columns[cat_cols] if len(cat_cols) > 0 else np.array([])\n",
        "            self.numerical_enc.cols = X_columns[num_cols] if len(num_cols) > 0 else np.array([])\n",
        "            self.datetime_enc.cols = X_columns[dt_cols] if len(dt_cols) > 0 else np.array([])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # target encoding\n",
        "        y_copy = None\n",
        "        if y is not None:\n",
        "            X_result, y_copy = self.fill_omissions(X, y)\n",
        "            if self.target_encoding:\n",
        "                if self.target_encoder in self._target_encoders_list:\n",
        "                    y_copy = self._target_encoders_list[self.target_encoder].fit_transform(y_copy)\n",
        "                else:\n",
        "                    y_copy = skpr.LabelEncoder().fit_transform(y_copy)\n",
        "        else:\n",
        "            X_result = self.fill_omissions(X)\n",
        "\n",
        "        # main encoding\n",
        "        if not isinstance(X_result, pd.DataFrame):\n",
        "            X_result = pd.DataFrame(X_result)\n",
        "\n",
        "        if self.numerical_enabled:\n",
        "            X_result = self.numerical_enc.transform(X_result, y_copy)\n",
        "\n",
        "        if self.categorical_enabled:\n",
        "            X_result = self.categorical_enc.transform(X_result, y_copy)\n",
        "\n",
        "        if self.datetime_enabled:\n",
        "            X_result = self.datetime_enc.transform(X_result, y_copy)\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X_result = X_result.to_numpy()\n",
        "\n",
        "        # getting the result\n",
        "        if self.target_encoding:\n",
        "            return X_result, y_copy\n",
        "        else:\n",
        "            return X_result\n",
        "\n",
        "    def fill_omissions(self, X, y=None):\n",
        "        if y is not None:\n",
        "            return copy.deepcopy(X), copy.deepcopy(y)\n",
        "        else:\n",
        "            return copy.deepcopy(X)\n",
        "\n",
        "    def print_cols(self):\n",
        "        print('Categorical columns: ', sep=', ', end='')\n",
        "        print(*self.categorical_enc.get_cols(), sep=', ')\n",
        "        print('Numerical columns: ', sep=', ', end='')\n",
        "        print(*self.numerical_enc.get_cols(), sep=', ')\n",
        "        print('Datetime columns: ', sep=', ', end='')\n",
        "        print(*self.datetime_enc.get_cols(), sep=', ')\n",
        "\n",
        "    def cols_to_numeric(self, X, cols):\n",
        "        if cols is None or not cols.shape[0]:\n",
        "            return np.array([])\n",
        "\n",
        "        inds = cols.copy()\n",
        "\n",
        "        try:\n",
        "            inds = inds.astype(int)\n",
        "            return inds\n",
        "        except (ValueError, TypeError):\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                return np.array([])\n",
        "            else:\n",
        "                columns_array = X.columns.to_numpy(dtype=object).copy()\n",
        "                names_dict = dict((columns_array[i], i) for i in range(columns_array.shape[0]))\n",
        "                inds = np.array([names_dict[name] for name in inds if name in names_dict])\n",
        "                return inds\n",
        "\n",
        "    def get_cols(self):\n",
        "        return {'categorical': self.categorical_enc.get_cols(),\n",
        "                'numerical': self.numerical_enc.get_cols(),\n",
        "                'datetime': self.datetime_enc.get_cols()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82JYdo1MKto6"
      },
      "source": [
        "#### Отбор признаков (author: github.com/EnriFermi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Basic encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelectNullDistributionBaseline(BaseEstimator, TransformerMixin):\n",
        "  def __get_feature_importances(s, X, y, shuffle, seed=None):\n",
        "    f_num = X.shape[1]\n",
        "    if shuffle:\n",
        "        np.random.shuffle(y)\n",
        "\n",
        "    #TODO hardcode\n",
        "    dtrain = lgb.Dataset(X, y,params={'verbose': -1}, free_raw_data=False, silent=True)\n",
        "    lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'boosting_type': 'rf',\n",
        "        'subsample': 0.623,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'num_leaves': 127,\n",
        "        'max_depth': 8,\n",
        "        'seed': seed,\n",
        "        'bagging_freq': 1,\n",
        "        'n_jobs': 4,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    categorical_feats = []\n",
        "    # Fit the model\n",
        "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n",
        "\n",
        "    # Get feature importances\n",
        "    imp_df = pd.DataFrame()\n",
        "    imp_df[\"feature\"] = np.arange(f_num)\n",
        "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
        "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
        "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(X))\n",
        "\n",
        "    return imp_df\n",
        "\n",
        "  def __get_null_feature_importances(s, X, y):\n",
        "    null_imp_df = pd.DataFrame()\n",
        "    dsp = ''\n",
        "    for i in range(s.n_runs):\n",
        "        X_train, y_train = resample(X, y)\n",
        "        imp_df = s.__get_feature_importances(X_train, y_train, shuffle=True)\n",
        "        imp_df['run'] = i + 1\n",
        "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
        "    return null_imp_df\n",
        "  \n",
        "  def __create_mask(s):\n",
        "    n_feats = len(s.feat_scores)\n",
        "    n_feat_selected = n_feats if s.max_features is None else min(s.max_features, n_feats)\n",
        "    if s.threshold == 'median':\n",
        "      s.feat_mask = s.feat_scores >= np.median(s.feat_scores)\n",
        "    elif s.threshold == 'mean':\n",
        "      s.feat_mask = s.feat_scores >= np.mean(s.feat_scores)\n",
        "    else:\n",
        "      s.feat_mask = s.feat_scores >= s.threshold\n",
        "    \n",
        "    s.feats_ind = np.arange(n_feats)[s.feat_mask][:n_feat_selected]\n",
        "\n",
        "  def __init__(self, typ = 'class', threshold=None, max_features=None, n_runs=80, n_bins=20, n_gauss_components=5, score_func=(lambda x: 0.5 * x) ):\n",
        "    self.typ = typ\n",
        "    self.threshold = threshold if threshold is not None else 'median'\n",
        "    self.max_features = max_features\n",
        "    self.n_runs = n_runs\n",
        "    self.n_bins = n_bins\n",
        "    self.n_gauss_components = n_gauss_components\n",
        "    self.score_func = score_func\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    actual_imp_df = self.__get_feature_importances(X, y, shuffle=False)\n",
        "    null_imp_df = self.__get_null_feature_importances(X, y)\n",
        "\n",
        "    self.feat_scores = []\n",
        "    for feat in range(X.shape[1]):\n",
        "      actual_feat_imp = actual_imp_df.loc[actual_imp_df['feature'] == feat]\n",
        "      null_feat_imp = null_imp_df.loc[null_imp_df['feature'] == feat]\n",
        "      actual_imp = actual_feat_imp['importance_gain'].iloc[0]\n",
        "      null_imp = null_feat_imp['importance_gain']\n",
        "      #TODO choose bin strategy\n",
        "      # act_len = np.max(actual_imp) - np.min(actual_imp)\n",
        "      # null_len  = np.max(null_imp) - np.min(null_imp)\n",
        "      # total_len = max(np.max(null_imp), np.max(actual_imp)) - min(np.min(null_imp), np.min(actual_imp))\n",
        "      # bin_count = max(total_len // null_len * n_bins, total_len // act_len * n_bins)\n",
        "      _sf_p = np.percentile(null_imp, 75)\n",
        "      print(_sf_p, actual_imp)\n",
        "      # print(actual_imp)\n",
        "      # print(null_imp)\n",
        "      # print(substr_imp)\n",
        "      # plt.hist(substr_imp, bins=20, label='diff', density=True)\n",
        "      # plt.hist(null_imp, bins=20, label='null', density=True)\n",
        "      # plt.hist(actual_imp, bins=20, label='actual', density=True)\n",
        "      # plt.legend(loc='upper right')\n",
        "      # plt.show()\n",
        "\n",
        "      feat_score = np.log(actual_imp/_sf_p)\n",
        "      print('f', feat_score, 'g', type(feat_score))\n",
        "      self.feat_scores.append(feat_score)\n",
        "    # print(self.feat_scores)\n",
        "    # print(np.array(self.feat_scores).argsort().argsort())\n",
        "    self.__create_mask()\n",
        "    \n",
        "  def transform(self, X):\n",
        "    return X[:, self.feats_ind]\n",
        "  def get_support(self, indices=False):\n",
        "    return self.feats_ind if indices else self.feat_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Modified encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j_k_sr5bkvf6"
      },
      "outputs": [],
      "source": [
        "class SelectNullDistribution(BaseEstimator, TransformerMixin):\n",
        "  def __get_feature_importances(s, X, y, shuffle, seed=None):\n",
        "    f_num = X.shape[1]\n",
        "    if shuffle:\n",
        "        np.random.shuffle(y)\n",
        "\n",
        "    #TODO hardcode\n",
        "    dtrain = lgb.Dataset(X, y,params={'verbose': -1}, free_raw_data=False, silent=True)\n",
        "    lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'boosting_type': 'rf',\n",
        "        'subsample': 0.623,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'num_leaves': 127,\n",
        "        'max_depth': 8,\n",
        "        'seed': seed,\n",
        "        'bagging_freq': 1,\n",
        "        'n_jobs': 4,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    categorical_feats = []\n",
        "    # Fit the model\n",
        "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n",
        "\n",
        "    # Get feature importances\n",
        "    imp_df = pd.DataFrame()\n",
        "    imp_df[\"feature\"] = np.arange(f_num)\n",
        "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
        "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
        "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(X))\n",
        "\n",
        "    return imp_df\n",
        "  \n",
        "  def __get_real_feature_importances(s, X, y):\n",
        "    real_imp_df = pd.DataFrame()\n",
        "    dsp = ''\n",
        "    for run in range(s.n_runs):\n",
        "      # skf = StratifiedKFold(n_splits=n_splits, random_state=None, shuffle=True)\n",
        "      # for i, (train_index, _) in enumerate(skf.split(X, y)):\n",
        "      #     imp_df = s.__get_feature_importances(X[train_index, :], y[train_index], shuffle=False)\n",
        "      #     imp_df['run'] = run * n_splits + i + 1\n",
        "      #     real_imp_df = pd.concat([real_imp_df, imp_df], axis=0)\n",
        "      X_train, y_train = resample(X, y)\n",
        "      imp_df = s.__get_feature_importances(X_train, y_train, shuffle=False)\n",
        "      imp_df['run'] = run + 1\n",
        "      real_imp_df = pd.concat([real_imp_df, imp_df], axis=0)\n",
        "    return real_imp_df\n",
        "\n",
        "  def __get_null_feature_importances(s, X, y):\n",
        "    null_imp_df = pd.DataFrame()\n",
        "    dsp = ''\n",
        "    for i in range(s.n_runs):\n",
        "        X_train, y_train = resample(X, y)\n",
        "        imp_df = s.__get_feature_importances(X_train, y_train, shuffle=True)\n",
        "        imp_df['run'] = i + 1\n",
        "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
        "    return null_imp_df\n",
        "  \n",
        "  def __create_mask(s):\n",
        "    n_feats = len(s.feat_scores)\n",
        "    n_feat_selected = n_feats if s.max_features is None else min(s.max_features, n_feats)\n",
        "    if s.threshold == 'median':\n",
        "      s.feat_mask = s.feat_scores >= np.median(s.feat_scores)\n",
        "    elif s.threshold == 'mean':\n",
        "      s.feat_mask = s.feat_scores >= np.mean(s.feat_scores)\n",
        "    else:\n",
        "      s.feat_mask = s.feat_scores >= s.threshold\n",
        "    \n",
        "    s.feats_ind = np.arange(n_feats)[s.feat_mask][:n_feat_selected]\n",
        "\n",
        "  def __init__(self, typ = 'class', threshold=None, max_features=None, n_runs=300, n_bins=20, n_gauss_components=5, score_func=(lambda x: 0.5 * x) ):\n",
        "    self.typ = typ\n",
        "    self.threshold = threshold if threshold is not None else 'median'\n",
        "    self.max_features = max_features\n",
        "    self.n_runs = n_runs\n",
        "    self.n_bins = n_bins\n",
        "    self.n_gauss_components = n_gauss_components\n",
        "    self.score_func = score_func\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    actual_imp_df = self.__get_real_feature_importances(X, y)\n",
        "    null_imp_df = self.__get_null_feature_importances(X, y)\n",
        "\n",
        "    self.feat_scores = []\n",
        "    for feat in range(X.shape[1]):\n",
        "      actual_feat_imp = actual_imp_df.loc[actual_imp_df['feature'] == feat]\n",
        "      null_feat_imp = null_imp_df.loc[null_imp_df['feature'] == feat]\n",
        "      actual_imp = actual_feat_imp['importance_gain']\n",
        "      null_imp = null_feat_imp['importance_gain']\n",
        "      #TODO choose bin strategy\n",
        "      # act_len = np.max(actual_imp) - np.min(actual_imp)\n",
        "      # null_len  = np.max(null_imp) - np.min(null_imp)\n",
        "      # total_len = max(np.max(null_imp), np.max(actual_imp)) - min(np.min(null_imp), np.min(actual_imp))\n",
        "      # bin_count = max(total_len // null_len * n_bins, total_len // act_len * n_bins)\n",
        "      substr_imp = np.sum(np.array(np.meshgrid(actual_imp, -null_imp)).T.reshape(-1,2), axis=1)\n",
        "      # print(actual_imp)\n",
        "      # print(null_imp)\n",
        "      # print(substr_imp)\n",
        "      # plt.hist(substr_imp, bins=20, label='diff', density=True)\n",
        "      # plt.hist(null_imp, bins=20, label='null', density=True)\n",
        "      # plt.hist(actual_imp, bins=20, label='actual', density=True)\n",
        "      # plt.legend(loc='upper right')\n",
        "      # plt.show()\n",
        "      gm_model = GaussianMixture(n_components=self.n_gauss_components)\n",
        "      gm_model.fit(substr_imp.reshape(-1, 1))\n",
        "      means_ = gm_model.means_\n",
        "      score_func_vf = np.vectorize(self.score_func)\n",
        "      feat_score = np.sum(score_func_vf(means_))\n",
        "      self.feat_scores.append(feat_score)\n",
        "    # print(self.feat_scores)\n",
        "    # print(np.array(self.feat_scores).argsort().argsort())\n",
        "    self.__create_mask()\n",
        "    \n",
        "  def transform(self, X):\n",
        "    return X[:, self.feats_ind]\n",
        "  def get_support(self, indices=False):\n",
        "    return self.feats_ind if indices else self.feat_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### BaseSelection encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FDZ-WHV_sZgp"
      },
      "outputs": [],
      "source": [
        "class FeatureSelectionTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, typ='skb_mutual_info_classif', k=50, task_type = 'class', n_estimators = 500):\n",
        "      if task_type == 'class':\n",
        "        if typ[0:3] == 'skb':\n",
        "            func = getattr(fs, typ[4:])\n",
        "            self.predictor = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "        elif typ == 'rtree':\n",
        "            self.predictor = SelectFromModel(ensemble.RandomForestClassifier(n_estimators = n_estimators), max_features=k)\n",
        "        elif typ == 'lsvm':\n",
        "            self.predictor = SelectFromModel(svm.LinearSVC(C=0.1, penalty=\"l1\", dual=False), max_features=k)\n",
        "        elif typ == 'null_importance':\n",
        "            self.predictor = SelectNullDistributionBaseline()\n",
        "        elif typ == 'null_importance_mod':\n",
        "            self.predictor = SelectNullDistribution()\n",
        "      elif task_type == 'reg':\n",
        "        if typ[0:2] == 'skb':\n",
        "            func = getattr(fs, typ[3:])\n",
        "            self.predictor = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self.predictor.fit(X, y)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.predictor.transform(X)\n",
        "\n",
        "    def get_support(self, indices):\n",
        "        return self.predictor.get_support(indices=indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWqk6TpQ_4e"
      },
      "source": [
        "#### Генерация признаков (author: github.com/EgorSWEB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QeYQ-_DNREhx"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.covariance import empirical_covariance\n",
        "from sklearn. preprocessing import normalize\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "EPS = 1e-20\n",
        "EPS_LOG = 1e-3\n",
        "THRESHOLD = 0.2\n",
        "\n",
        "class FeatureGenerationTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, thr=THRESHOLD, features_mask=None, important_features=None, stand_gen=True, corr_gen=True):\n",
        "        self.thr = thr\n",
        "        self._features_mask = features_mask\n",
        "        self._important_features = important_features\n",
        "        self._stand_gen = stand_gen\n",
        "        self._corr_gen = corr_gen\n",
        "        self.desc_dict = {}\n",
        "\n",
        "    #Construction of a matrix of correlation coefficients of features\n",
        "    def _correlation_create(self, X):\n",
        "        return np.corrcoef(X.T)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = np.array(X)\n",
        "\n",
        "        self._cnt_columns = X.shape[1]\n",
        "\n",
        "        if self._features_mask is None:\n",
        "            self._features_mask = np.arange(X.shape[1])\n",
        "        else:\n",
        "            self._features_mask = np.array(self._features_mask)\n",
        "\n",
        "        if self._important_features is None:\n",
        "            self._important_features = self._features_mask\n",
        "        else:\n",
        "            self._important_features = np.array(self._important_features)\n",
        "\n",
        "        if self._important_features.size < 2:\n",
        "            self._corr_gen = False\n",
        "\n",
        "        if self._corr_gen:\n",
        "            self._corr_mat = self._correlation_create(X[:, self._important_features])\n",
        "\n",
        "        if self._stand_gen:\n",
        "            self._cnt_columns = self._standard_generation(X, self._cnt_columns)\n",
        "        if self._corr_gen:\n",
        "            self._cnt_columns = self._correlation_generation(X, self._cnt_columns)\n",
        "\n",
        "        return self\n",
        "\n",
        "    #Generating features using standard functions\n",
        "    def _standard_generation(self, X, cnt_columns=None):\n",
        "        features_mask = self._features_mask\n",
        "        important_features = self._important_features\n",
        "\n",
        "        if cnt_columns is None:\n",
        "            cnt_columns = X.shape[1]\n",
        "\n",
        "        if features_mask.size:\n",
        "            #exponent\n",
        "            self.desc_dict['s_exp'] = [features_mask, np.arange(cnt_columns, cnt_columns + features_mask.shape[0])]\n",
        "            cnt_columns += features_mask.shape[0]\n",
        "\n",
        "            #x^2\n",
        "            self.desc_dict['s_^2'] = [features_mask, np.arange(cnt_columns, cnt_columns + features_mask.shape[0])]\n",
        "            cnt_columns += features_mask.shape[0]\n",
        "\n",
        "            #x^3\n",
        "            self.desc_dict['s_^3'] = [features_mask, np.arange(cnt_columns, cnt_columns + features_mask.shape[0])]\n",
        "            cnt_columns += features_mask.shape[0]\n",
        "\n",
        "\n",
        "        if important_features.size:\n",
        "            #logarithm\n",
        "            self.desc_dict['s_log'] = [important_features, np.arange(cnt_columns, cnt_columns + important_features.shape[0])]\n",
        "            cnt_columns += important_features.shape[0]\n",
        "\n",
        "            #x^0.5\n",
        "            self.desc_dict['s_^0.5'] = [important_features, np.arange(cnt_columns, cnt_columns + important_features.shape[0])]\n",
        "            cnt_columns += important_features.shape[0]\n",
        "\n",
        "        return cnt_columns\n",
        "\n",
        "    #Generating features from two that have a correlation coefficient less than the threshold\n",
        "    def _correlation_generation(self, X, cnt_columns=None):\n",
        "        if cnt_columns is None:\n",
        "            cnt_columns = X.shape[1]\n",
        "\n",
        "        important_features = self._important_features\n",
        "\n",
        "        pairs_indxs_mat = np.array(list(combinations(range(self._corr_mat.shape[0]), 2)))\n",
        "\n",
        "        #x1 * x2\n",
        "        self.desc_dict['p_*'] = [important_features[pairs_indxs_mat], np.arange(cnt_columns, cnt_columns + pairs_indxs_mat.shape[0])]\n",
        "        cnt_columns += pairs_indxs_mat.shape[0]\n",
        "\n",
        "        #x1 / x2, x2 / x1\n",
        "        self.desc_dict['p_/'] = [np.hstack([important_features[pairs_indxs_mat], important_features[pairs_indxs_mat][:,::-1]]).reshape(-1, 2),\n",
        "                               np.arange(cnt_columns, cnt_columns + 2 * pairs_indxs_mat.shape[0])]\n",
        "        cnt_columns += 2 * pairs_indxs_mat.shape[0]\n",
        "\n",
        "        pairs_indxs_mat = np.array([[[i, j] for j in range(self._corr_mat.shape[1])] for i in range(self._corr_mat.shape[0])])\n",
        "        pairs_indxs_mat = pairs_indxs_mat[abs(self._corr_mat) <= self.thr]\n",
        "        pairs_indxs_mat = pairs_indxs_mat[pairs_indxs_mat[:, 0] > pairs_indxs_mat[:, 1]]\n",
        "\n",
        "        if pairs_indxs_mat.size:\n",
        "            #x1 + x2\n",
        "            self.desc_dict['p_+'] = [important_features[pairs_indxs_mat], np.arange(cnt_columns, cnt_columns + pairs_indxs_mat.shape[0])]\n",
        "            cnt_columns += pairs_indxs_mat.shape[0]\n",
        "\n",
        "            #x1 - x2, x2 - x1\n",
        "            self.desc_dict['p_-'] = [np.hstack([important_features[pairs_indxs_mat], important_features[pairs_indxs_mat][:,::-1]]).reshape(-1, 2),\n",
        "                                   np.arange(cnt_columns, cnt_columns + 2 * pairs_indxs_mat.shape[0])]\n",
        "            cnt_columns += 2 * pairs_indxs_mat.shape[0]\n",
        "\n",
        "        return cnt_columns\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = np.array(X)\n",
        "\n",
        "        cnt_columns = 0\n",
        "        # print(self.desc_dict)\n",
        "        for k, v in self.desc_dict.items():\n",
        "            if k != '_':\n",
        "              cnt_columns += self.desc_dict[k][1].shape[0]\n",
        "\n",
        "        X = np.hstack([X, np.zeros((X.shape[0], cnt_columns))])\n",
        "\n",
        "        #exponent\n",
        "        if 's_exp' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['s_exp'][1]] =  np.exp(np.clip(X[:,self.desc_dict['s_exp'][0]], -750, 700))\n",
        "\n",
        "        #x^2\n",
        "        if 's_^2' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['s_^2'][1]] =  np.power(X[:,self.desc_dict['s_^2'][0]], 2)\n",
        "\n",
        "        #x^3\n",
        "        if 's_^3' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['s_^3'][1]] =  np.power(X[:,self.desc_dict['s_^3'][0]], 3)\n",
        "\n",
        "        #logarithm\n",
        "        if 's_log' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['s_log'][1]] =  np.where(X[:, self.desc_dict['s_log'][0]] <= -1, np.log(EPS_LOG),\n",
        "                                                         np.log(X[:,self.desc_dict['s_log'][0]] + 1, where=X[:,self.desc_dict['s_log'][0]] > -1))\n",
        "\n",
        "        #x^0.5\n",
        "        if 's_^0.5' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['s_^0.5'][1]] = np.where(X[:,self.desc_dict['s_^0.5'][0]] < 0, -np.power(-X[:,self.desc_dict['s_^0.5'][0]], 0.5,\n",
        "                                                          where=X[:,self.desc_dict['s_^0.5'][0]] < 0), np.power(X[:,self.desc_dict['s_^0.5'][0]], 0.5,\n",
        "                                                          where=X[:,self.desc_dict['s_^0.5'][0]] >= 0))\n",
        "        #x1 * x2\n",
        "        if 'p_*' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['p_*'][1]] = np.prod(X.T[self.desc_dict['p_*'][0]], axis=1).T\n",
        "\n",
        "        #x1 / x2, x2 / x1\n",
        "        if 'p_/' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['p_/'][1]] = X.T[self.desc_dict['p_/'][0][:, 0]].T / (X.T[self.desc_dict['p_/'][0][:, 1]].T + EPS)\n",
        "\n",
        "        #x1 + x2\n",
        "        if 'p_+' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['p_+'][1]] = np.sum(X.T[self.desc_dict['p_+'][0]], axis=1).T\n",
        "\n",
        "        #x1 - x2, x2 - x1\n",
        "        if 'p_-' in self.desc_dict.keys():\n",
        "            X[:, self.desc_dict['p_-'][1]] = X.T[self.desc_dict['p_-'][0][:, 0]].T - X.T[self.desc_dict['p_-'][0][:, 1]].T\n",
        "\n",
        "        INF = 1e30\n",
        "        X = np.clip(X, -INF, INF)\n",
        "        return X\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     X, y = make_classification(\n",
        "#     n_samples=100000, n_features=100, n_informative=80, n_redundant=2,\n",
        "#     random_state=42)\n",
        "\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, test_size=0.2, random_state=42)\n",
        "#     scaler = StandardScaler()\n",
        "\n",
        "#     result = scaler.fit_transform(X_train)\n",
        "#     feat_gen = FeatureGenerationTransformer(thr=0.01,  important_features=[1, 2, 3, 4, 5, 6, 7, 8, 9, 92])\n",
        "#     start_time = time.time()\n",
        "#     result_2 = feat_gen.fit_transform(result)\n",
        "#     end_time = time.time()\n",
        "#     elapsed_time = end_time - start_time\n",
        "#     print(f'Время генерации: {elapsed_time:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGbWhOMIKzQ2"
      },
      "source": [
        "#### Алгоритм отбора признаков с генерацией (author: github.com/EgorSWEB, github.com/EnriFermi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jQcLzl0WJ8W5"
      },
      "outputs": [],
      "source": [
        "def merge_desc_dict(dict1, dict2, bias=0):\n",
        "  rdict = {}\n",
        "  pointer = bias\n",
        "  for op in dict1.keys():\n",
        "    val = np.unique(np.concatenate((dict1[op][0],dict2[op][0]), axis=0), axis=0)\n",
        "    rdict[op] = [val, np.arange(pointer, pointer+val.shape[0])]\n",
        "    if op != \"_\":\n",
        "      pointer += val.shape[0]\n",
        "  return rdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatProcessingMod(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, task_type, n_fold_splits, gen_kwargs={}, sel_for_gen_kwargs={}, sel_from_gen_kwargs={}, for_k_limit=50):\n",
        "      self.n_fold_splits = n_fold_splits\n",
        "      self.gen_kwargs = gen_kwargs\n",
        "      self.sel_for_gen_kwargs = sel_for_gen_kwargs\n",
        "      self.sel_from_gen_kwargs = sel_from_gen_kwargs\n",
        "      self.for_k_limit = for_k_limit\n",
        "\n",
        "      self.desc_dict = {}\n",
        "    def fit(self, X, y=None):\n",
        "      print(X.shape)\n",
        "\n",
        "\n",
        "      if 'k' not in self.sel_for_gen_kwargs.keys():\n",
        "        for_k = min(X.shape[1], self.for_k_limit)\n",
        "        self.sel_for_gen_kwargs['k'] = for_k\n",
        "      else:\n",
        "        for_k = self.sel_for_gen_kwargs['k']\n",
        "\n",
        "      if 'k' not in self.sel_from_gen_kwargs.keys():\n",
        "        from_k = X.shape[1]\n",
        "        self.sel_from_gen_kwargs['k'] = from_k\n",
        "        # self.sel_from_gen_kwargs['thr'] = -np.inf\n",
        "      else:\n",
        "        from_k = self.sel_from_gen_kwargs['k']\n",
        "\n",
        "\n",
        "      kf = KFold(n_splits=self.n_fold_splits)\n",
        "\n",
        "      for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "\n",
        "        X_train, X_val, y_train, y_val = X[train_index, :], X[val_index, :], y[train_index], y[val_index]\n",
        "        obj_count, f_dim = X_train.shape[0], X_train.shape[1]\n",
        "        #1 SEL\n",
        "        self.sel_for_gen = FeatureSelectionTransformer(**self.sel_for_gen_kwargs)\n",
        "        self.sel_for_gen.fit(X_train, y_train)\n",
        "        important_features = self.sel_for_gen.get_support(indices=True)[:for_k]\n",
        "\n",
        "        # print('for_k:', for_k)\n",
        "        #Константа просчитывается с учетом того, что под вычисления отводится 8 гб ОЗУ\n",
        "        for_k_limit_important = int(((1073741824 / (X.shape[0] / 0.8) - X.shape[1] - for_k * 9) / 9) ** 0.5)\n",
        "        # print('for_k_limit_important:', for_k_limit_important)\n",
        "        # print('abs:', abs(min(for_k, for_k_limit_important)))\n",
        "\n",
        "        #GEN\n",
        "        self.gen = FeatureGenerationTransformer(features_mask=important_features, important_features=important_features[:min(abs(for_k), abs(for_k_limit_important))], **self.gen_kwargs)\n",
        "        self.gen.fit(X_train, y_train)\n",
        "        X_gen = self.gen.transform(X_val)\n",
        "\n",
        "        pipe_sc = pipeline.Pipeline([('std_scaler',preprocessing.StandardScaler())])\n",
        "        X_gen[:, X_val.shape[1]:] = pipe_sc.fit_transform(X_gen[:,  X_val.shape[1]:])\n",
        "        #2 SEL\n",
        "        self.sel_from_gen = FeatureSelectionTransformer(**self.sel_from_gen_kwargs)\n",
        "        self.sel_from_gen.fit(X_gen, y_val)\n",
        "        #Prepare indicies\n",
        "        fos = important_features # индексы в исходном векторе,\n",
        "        # выбранных признаков\n",
        "        frs = self.sel_from_gen.get_support(indices=True)[:from_k] # индексы в сгенерированном векторе,\n",
        "        # выбранных признаков\n",
        "\n",
        "\n",
        "        desc =  self.gen.desc_dict\n",
        "        # print(desc)\n",
        "        desc['_'] = [np.arange(0, f_dim), np.arange(0, f_dim)]\n",
        "        sort_desc = {}\n",
        "        map = np.zeros(X_gen.shape[0])\n",
        "\n",
        "        #for keys\n",
        "        for op in desc.keys():\n",
        "           #Начинается с 0\n",
        "          # print(frs, fos, desc[op][0])\n",
        "          mask = np.in1d(desc[op][1], frs) #remove generating which are not selected\n",
        "          sort_desc[op] = [desc[op][0][mask], desc[op][1][mask]]\n",
        "          # print(sort_desc[op])\n",
        "\n",
        "          # if op != '_':\n",
        "          #   sort_desc[op][0] = fos[sort_desc[op][0]]\n",
        "        if len(self.desc_dict.keys()) == 0:\n",
        "          self.desc_dict = sort_desc\n",
        "        else:\n",
        "          self.desc_dict = merge_desc_dict(self.desc_dict, sort_desc, bias=f_dim) # union algorythms\n",
        "        # print(self.desc_dict)\n",
        "\n",
        "      self.gen.desc_dict = self.desc_dict #!!!!!\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "      X_res = self.gen.transform(X)\n",
        "\n",
        "      # for i in list(set(range(X.shape[1])) - set(self.desc_dict['_'][0])):\n",
        "\n",
        "      #   X_res = np.delete(X_res, i, 1)\n",
        "      return X_res\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b-wTUzlqJkph"
      },
      "outputs": [],
      "source": [
        "class FeatProcessing(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, task_type, n_fold_splits, gen_kwargs={}, sel_for_gen_kwargs={}, sel_from_gen_kwargs={}):\n",
        "      self.n_fold_splits = n_fold_splits\n",
        "      self.gen_kwargs = gen_kwargs\n",
        "      self.sel_for_gen_kwargs = sel_for_gen_kwargs\n",
        "      self.sel_from_gen_kwargs = sel_from_gen_kwargs\n",
        "\n",
        "      self.desc_dict = {}\n",
        "    def fit(self, X, y=None):\n",
        "      print(X.shape)\n",
        "\n",
        "\n",
        "      if 'k' not in self.sel_for_gen_kwargs.keys():\n",
        "        for_k = X.shape[1] // 2\n",
        "        self.sel_for_gen_kwargs['k'] = for_k\n",
        "      else:\n",
        "        for_k = self.sel_for_gen_kwargs['k']\n",
        "\n",
        "      if 'k' not in self.sel_from_gen_kwargs.keys():\n",
        "        from_k = X.shape[1]\n",
        "        self.sel_for_gen_kwargs['k'] = from_k\n",
        "      else:\n",
        "        from_k = self.sel_from_gen_kwargs['k']\n",
        "\n",
        "\n",
        "      kf = KFold(n_splits=self.n_fold_splits)\n",
        "\n",
        "      for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "\n",
        "        X_train, X_val, y_train, y_val = X[train_index, :], X[val_index, :], y[train_index], y[val_index]\n",
        "        obj_count, f_dim = X_train.shape[0], X_train.shape[1]\n",
        "        #1 SEL\n",
        "        self.sel_for_gen = FeatureSelectionTransformer(**self.sel_for_gen_kwargs)\n",
        "        self.sel_for_gen.fit(X_train, y_train)\n",
        "        important_features = self.sel_for_gen.get_support(indices=True)[:for_k]\n",
        "        #GEN\n",
        "        self.gen = FeatureGenerationTransformer(features_mask=important_features, important_features=important_features, **self.gen_kwargs)\n",
        "        self.gen.fit(X_train, y_train)\n",
        "        X_gen = self.gen.transform(X_val)\n",
        "\n",
        "        #2 SEL\n",
        "        self.sel_from_gen = FeatureSelectionTransformer(**self.sel_from_gen_kwargs)\n",
        "        self.sel_from_gen.fit(X_gen, y_val)\n",
        "        #Prepare indicies\n",
        "        fos = important_features # индексы в исходном векторе,\n",
        "        # выбранных признаков\n",
        "        frs = self.sel_from_gen.get_support(indices=True)[:from_k] # индексы в сгенерированном векторе,\n",
        "        # выбранных признаков\n",
        "\n",
        "\n",
        "        desc =  self.gen.desc_dict\n",
        "        # print(desc)\n",
        "        desc['_'] = [np.arange(0, f_dim), np.arange(0, f_dim)]\n",
        "        sort_desc = {}\n",
        "        map = np.zeros(X_gen.shape[0])\n",
        "\n",
        "        #for keys\n",
        "        for op in desc.keys():\n",
        "           #Начинается с 0\n",
        "          # print(frs, fos, desc[op][0])\n",
        "          mask = np.in1d(desc[op][1], frs) #remove generating which are not selected\n",
        "          sort_desc[op] = [desc[op][0][mask], desc[op][1][mask]]\n",
        "          # print(sort_desc[op])\n",
        "\n",
        "          # if op != '_':\n",
        "          #   sort_desc[op][0] = fos[sort_desc[op][0]]\n",
        "        if len(self.desc_dict.keys()) == 0:\n",
        "          self.desc_dict = sort_desc\n",
        "        else:\n",
        "          self.desc_dict = merge_desc_dict(self.desc_dict, sort_desc, bias=f_dim) # union algorythms\n",
        "        # print(self.desc_dict)\n",
        "\n",
        "      self.gen.desc_dict = self.desc_dict #!!!!!\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "      X_res = self.gen.transform(X)\n",
        "\n",
        "      # for i in list(set(range(X.shape[1])) - set(self.desc_dict['_'][0])):\n",
        "\n",
        "      #   X_res = np.delete(X_res, i, 1)\n",
        "      return X_res\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i59fvOeZhOYj"
      },
      "source": [
        "#### Final build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "olXaxHhbJhas"
      },
      "outputs": [],
      "source": [
        "class ValidTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, task_type, **kwargs):\n",
        "      self.task_type = task_type\n",
        "      self.kwargs = kwargs\n",
        "      if 'work_time' in kwargs.keys():\n",
        "        self.work_time = kwargs['work_time']\n",
        "      else:\n",
        "        self.work_time = 1\n",
        "      if 'val_size' in kwargs.keys():\n",
        "        self.val_size = kwargs['val_size']\n",
        "      else:\n",
        "        self.val_size = 0.2\n",
        "      if 'random_state' in kwargs.keys():\n",
        "        self.random_state = kwargs['random_state']\n",
        "      else:\n",
        "        self.random_state = 42\n",
        "    def fit(self, X, y=None):\n",
        "      obj_count, f_dim = X.shape[0], X.shape[1]\n",
        "\n",
        "      if 'encoder' in self.kwargs.keys():\n",
        "        self.encoder = CategoricalEncoder(**self.kwargs['encoder'])\n",
        "      else:\n",
        "        self.encoder = CategoricalEncoder()\n",
        "      self.encoder = preprocessing.StandardScaler() # TODO add normalization after generation\n",
        "      if 'processor' in self.kwargs.keys():\n",
        "        self.trans = FeatProcessing(**self.kwargs['processor'])\n",
        "      else:\n",
        "        self.trans = FeatProcessing(self.task_type, 2)\n",
        "      # self.encoder.fit(X, y)\n",
        "      self.trans.fit(X, y)\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      # Perform arbitary transformation\n",
        "      X = self.encoder.transform(X)\n",
        "      return self.trans.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRcYsrDK7Pf"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WydREVsLvK6"
      },
      "source": [
        "##### Try with AutoML Banchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A4Dy7Slf3iZ",
        "outputId": "b5bd7687-e8d6-4d2e-abc5-cef90e6a4b19"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openml/automlbenchmark.git --branch stable --depth 1\n",
        "!cd automlbenchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDQomVcrgA8b",
        "outputId": "bda04f6a-4dd5-430b-dcc1-e90ee4633fac"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install -r automlbenchmark/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0z4GiC8g_lw",
        "outputId": "60ba8602-beb0-476c-b52c-b1352125a436"
      },
      "outputs": [],
      "source": [
        "!python automlbenchmark/runbenchmark.py benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHP055wEL1VK"
      },
      "source": [
        "##### Testing with OpenML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_accuracy(X, y, X_t, y_t):\n",
        "  models = [('linear_svm', svm.LinearSVC(max_iter=2000, random_state=42), 'decision_function'),\n",
        "            ('logistic', linear_model.LogisticRegression(solver = 'lbfgs', random_state=42), 'predict_proba'),\n",
        "            ('linear_discriminant_analysis', LinearDiscriminantAnalysis(), 'predict_proba'),\n",
        "            ('gradboost', ensemble.GradientBoostingClassifier(random_state=42), 'predict_proba'),\n",
        "            ('randomforest', ensemble.RandomForestClassifier(random_state=42), 'predict_proba')]\n",
        "  ans = {}\n",
        "  for name, model, loss_func in models:\n",
        "      model.fit(X, y)\n",
        "      # print(name, ': ', model.score(X_t, y_t))\n",
        "      ans[name] = model.score(X_t, y_t)\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XryzdH4AL31X",
        "outputId": "dc988cd1-2234-433a-8300-f38aceddd1d3"
      },
      "outputs": [],
      "source": [
        "!pip install openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuKyzUcZpxG3",
        "outputId": "d224df9e-a14d-4a23-e23b-6ba12d7b8fbf"
      },
      "outputs": [],
      "source": [
        "import openml\n",
        "\n",
        "dataset = openml.datasets.get_dataset(1471)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiXFxZet7ra1",
        "outputId": "d1749531-f6a3-4601-ca96-7a3bdb76c255"
      },
      "outputs": [],
      "source": [
        "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "frUlbMoV5ZcJ"
      },
      "outputs": [],
      "source": [
        "# Получение меток классов и их строковое представление\n",
        "class_labels = set(y)\n",
        "# Создание словаря для преобразования строковых меток в числовые индексы\n",
        "class_labels_map = {label: i for i, label in enumerate(class_labels)}\n",
        "y_numeric = [class_labels_map[label] for label in y]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), np.array(y_numeric), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PsaH3_XRcruq"
      },
      "outputs": [],
      "source": [
        "pipe_sc = pipeline.Pipeline([('std_scaler',preprocessing.StandardScaler())])\n",
        "pipe_sc.fit(X_train, y_train)\n",
        "X_train = pipe_sc.transform(X_train)\n",
        "X_test = pipe_sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans_full = FeatProcessingMod('class', 2, {'thr': 0.5},\n",
        "                       sel_for_gen_kwargs={'typ': 'null_importance'}, sel_from_gen_kwargs={})\n",
        "trans_full.fit(X_train, y_train)\n",
        "# print(X_train.shape)\n",
        "X_train_chgd_norm = trans_full.transform(X_train)\n",
        "# print(X_traоin.shape)\n",
        "X_test_chgd_norm = trans_full.transform(X_test)\n",
        "calc_accuracy(X_train_chgd_norm, y_train, X_test_chgd_norm, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_chgd_norm = trans_full.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_accuracy(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trans = FeatProcessing('class', 2, {'thr': 0.5},\n",
        "#                        sel_for_gen_kwargs={'typ': 'lsvm'}, sel_from_gen_kwargs={})\n",
        "trans_null = FeatureSelectionTransformer(typ='null_importance')\n",
        "trans_null.fit(X_train, y_train)\n",
        "\n",
        "trans_null_mod = FeatureSelectionTransformer(typ='null_importance_mod')\n",
        "trans_null_mod.fit(X_train, y_train)\n",
        "trans_baseline = FeatureSelectionTransformer(typ='rtree', k = X_train.shape[1])\n",
        "trans_baseline.fit(X_train, y_train)\n",
        "print(trans_baseline.predictor.estimator_.feature_importances_)\n",
        "print(trans_baseline.predictor.estimator_.feature_importances_.argsort().argsort())\n",
        "\n",
        "# print(X_train.shape)\n",
        "X_train_chgd_null_mod = trans_null_mod.transform(X_train)\n",
        "# print(X_train.shape)\n",
        "X_test_chgd_null_mod = trans_null_mod.transform(X_test)\n",
        "\n",
        "X_train_chgd_base = trans_baseline.transform(X_train)\n",
        "# print(X_train.shape)\n",
        "X_test_chgd_base = trans_baseline.transform(X_test)\n",
        "\n",
        "X_train_chgd_null = trans_null.transform(X_train)\n",
        "X_test_chgd_null = trans_null.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_accuracy(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_chgd_null, y_train, X_test_chgd_null, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_chgd_null_mod, y_train, X_test_chgd_null_mod, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_test_chgd_base.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_chgd_base, y_train, X_test_chgd_base, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7cC-O6HNx_X",
        "outputId": "92823d81-933d-4dd1-9a1c-8b689c3cccfa"
      },
      "outputs": [],
      "source": [
        "trans_null_mod = FeatProcessing('class', 2, {'thr': 0.5},\n",
        "                       sel_for_gen_kwargs={'typ': 'lsvm'}, sel_from_gen_kwargs={})\n",
        "trans_null_mod.fit(X_train, y_train)\n",
        "# print(X_train.shape)\n",
        "X_train_chgd = trans_null_mod.transform(X_train)\n",
        "# print(X_train.shape)\n",
        "X_test_chgd = trans_null_mod.transform(X_test)\n",
        "calc_accuracy(X_train_chgd, y_train, X_test_chgd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j16Wm4w74z1d",
        "outputId": "5a71aa37-4c0d-4e8c-e8d7-829fd66e59e6"
      },
      "outputs": [],
      "source": [
        "X_train_chgd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkxbUVCC4GEc",
        "outputId": "2658cad2-62c0-4320-bd7b-789bc1b090f9"
      },
      "outputs": [],
      "source": [
        "X_train_chgd[:3, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEL7dKUg4Nw6",
        "outputId": "50db938b-f351-49af-a860-00f419909a51"
      },
      "outputs": [],
      "source": [
        "X_train[:3, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93rEGzyTsWGE"
      },
      "outputs": [],
      "source": [
        "# Получение меток классов и их строковое представление\n",
        "class_labels = set(y)\n",
        "# Создание словаря для преобразования строковых меток в числовые индексы\n",
        "class_labels_map = {label: i for i, label in enumerate(class_labels)}\n",
        "y_numeric = [class_labels_map[label] for label in y]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_numeric, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "1--50xf6s_-K"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X, y, X_t, y_t):\n",
        "  models = [('linear_svm', svm.LinearSVC(max_iter=2000, random_state=42), 'decision_function'), ('ridge', linear_model.Ridge(random_state=42), 'predict'),\n",
        "            ('gradboost', ensemble.GradientBoostingClassifier(random_state=42), 'predict_proba'), ('logistic', linear_model.LogisticRegression(tol = 1e-4, solver = 'newton-cholesky', random_state=42), 'predict_proba'),\n",
        "            ('randomforest', ensemble.RandomForestClassifier(random_state=42), 'predict_proba')]\n",
        "  for name, model, loss_func in models:\n",
        "    model.fit(X, y)\n",
        "    print(name, ': ', model.score(X_t, y_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg4HYIczAWDS",
        "outputId": "3f98cbac-dcf8-4e7b-81ad-a42589ef65ea"
      },
      "outputs": [],
      "source": [
        "calc_accuracy(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7l4zvbJyaBd",
        "outputId": "23293d76-e6bf-4bbb-d9ef-b1e4224e0f42"
      },
      "outputs": [],
      "source": [
        "feat_gen = FeatureGenerationTransformer(thr=0.5)\n",
        "start_time = time.time()\n",
        "X_train_gen = feat_gen.fit_transform(np.array(X_train))\n",
        "X_test_gen = feat_gen.transform(np.array(X_test))\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Время генерации: {elapsed_time:.5f}')\n",
        "print('Размер до генерации', X_train.shape)\n",
        "print('Размер после генерации', X_train_gen.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnvIxxUo9MCE"
      },
      "outputs": [],
      "source": [
        "INF = 1e30\n",
        "X_train_gen = np.clip(X_train_gen, -INF, INF)\n",
        "X_test_gen = np.clip(X_test_gen, -INF, INF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "tBHsJNCQ9EYc",
        "outputId": "dae1ae6a-b5f7-4ebe-bf7f-7c97f1561caa"
      },
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_gen, y_train, X_test_gen, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_co269jxaZIJ"
      },
      "source": [
        "linear_svm :  0.45714285714285713<br>\n",
        "ridge :  -3537520734.514072<br>\n",
        "gradboost :  0.8<br>\n",
        "logistic :  0.8571428571428571<br>\n",
        "randomforest :  0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgf4HwjXFO2b"
      },
      "outputs": [],
      "source": [
        "pipe_sc = pipeline.Pipeline([('std_scaler',preprocessing.StandardScaler())])\n",
        "pipe_sc.fit(X_train, y_train)\n",
        "X_train_sc = pipe_sc.transform(X_train)\n",
        "X_test_sc = pipe_sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKh2RJThFVjP",
        "outputId": "9db36723-32bd-49ef-d581-b9dde77050ce"
      },
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_sc, y_train, X_test_sc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjSQ3FjfEcOF"
      },
      "outputs": [],
      "source": [
        "pipe_m = pipeline.Pipeline([('std_scaler',preprocessing.StandardScaler()), ('feat_gen', FeatureGenerationTransformer(thr=0.5)), ('feat_sel', FeatureSelectionTransformer('rtree', X_train.shape[1]))])\n",
        "pipe_m.fit(X_train, y_train)\n",
        "X_train_m = pipe_m.transform(X_train)\n",
        "X_test_m = pipe_m.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P5eMB8yE8oO",
        "outputId": "96afb940-20ad-482d-aa0b-25a45fd74fff"
      },
      "outputs": [],
      "source": [
        "X_test_m.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeSR0_3aFDOS",
        "outputId": "4897a582-d916-4bd9-edb6-ae2c9ff3bca7"
      },
      "outputs": [],
      "source": [
        "calc_accuracy(X_train_m, y_train, X_test_m, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YCSZsPuwLKyE",
        "82JYdo1MKto6",
        "qGbWhOMIKzQ2",
        "i59fvOeZhOYj",
        "2WydREVsLvK6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
